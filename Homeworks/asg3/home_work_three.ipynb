{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"pytorchudac","language":"python","name":"pytorchudac"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"home_work_three.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ZpMT-LAuuzAx","colab_type":"text"},"source":["## Homework III Frederik Chettouh"]},{"cell_type":"code","metadata":{"id":"yOahfUZH-yHZ","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","import numpy as np\n","import torch.nn.functional as F\n","from torch import optim\n","\n","from google.colab import files\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vHblrJcM_OUx","colab_type":"code","outputId":"cccd0e21-b2b0-4748-d8fb-6c88a60981fd","executionInfo":{"status":"ok","timestamp":1587644395173,"user_tz":-120,"elapsed":46821,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}},"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":71}},"source":["text=files.upload()"],"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-2e2d3c0f-a34a-47cd-bb12-656d210768cb\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-2e2d3c0f-a34a-47cd-bb12-656d210768cb\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving anna.txt to anna.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5F9EBaAD-yHg","colab_type":"code","colab":{}},"source":["with open ('anna.txt') as file:\n","    text=file.read()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eddyr3xJ-yHk","colab_type":"code","colab":{}},"source":["chars = tuple(set(text))\n","int2char = dict(enumerate(chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"06tidcq_-yHq","colab_type":"code","colab":{}},"source":["encoded=np.array([char2int[char] for char in text])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JGaxo1vw-yHv","colab_type":"code","colab":{}},"source":["def one_hot_encode(encoded,length):\n","    start_array=np.zeros((encoded.size,length),dtype=np.float32)\n","    start_array[np.arange(start_array.shape[0]),encoded.flatten()]=1.\n","    start_array=start_array.reshape((*encoded.shape,length))\n","    return start_array"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XzUbzUBf-yHy","colab_type":"code","colab":{}},"source":["sparse_matrix=one_hot_encode(encoded,83)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oY2Bj82B-yH4","colab_type":"code","outputId":"64e7f928-04fb-4417-b976-5228991b39b7","executionInfo":{"status":"ok","timestamp":1587644421685,"user_tz":-120,"elapsed":1175,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["encoded.shape"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1985223,)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"o9JSjjN5-yH-","colab_type":"code","colab":{}},"source":["def get_batches(arr, batch_size, seq_length):\n","#     number of total chars=N*M*K-->K=Chars/N*M\n","    chars_per_batch=batch_size*seq_length\n","    n_batches=arr.shape[0]//chars_per_batch\n","    chars_to_keep=chars_per_batch*n_batches\n","    \n","    arr=arr[:chars_to_keep]\n","    arr=arr.reshape(batch_size,-1)\n","    \n","    for n in range(0, arr.shape[1], seq_length):\n","        x = arr[:, n:n+seq_length]\n","        y = np.zeros_like(x)\n","        try:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n","        except IndexError:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","        yield x, y\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rVQ6K8En-yIF","colab_type":"code","colab":{}},"source":["batches = get_batches(encoded, 8, 50)\n","x, y = next(batches)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHN4WBniGWtB","colab_type":"code","outputId":"aa0bd706-291f-470d-b700-fa9953cfdf2a","executionInfo":{"status":"ok","timestamp":1587644426741,"user_tz":-120,"elapsed":1073,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["one_hot_encode(x, 83).shape"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 50, 83)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"Ft0AVH-H-yIH","colab_type":"text"},"source":["## Defining the network"]},{"cell_type":"code","metadata":{"id":"y9TG3Lxp-yII","colab_type":"code","outputId":"e8b9d217-0877-46fa-c14d-2b4ff446b73f","executionInfo":{"status":"ok","timestamp":1587644428477,"user_tz":-120,"elapsed":1203,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# check if GPU is available\n","train_on_gpu = torch.cuda.is_available()\n","if(train_on_gpu):\n","    print('Training on GPU!')\n","else: \n","    print('No GPU available, training on CPU; consider making n_epochs very small.')"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Training on GPU!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9YUzTJv--yIL","colab_type":"text"},"source":["The LSTM architecture is as follows:\n","- every character is passed to a two layer hidden Neural Network \n","- The two layers receive the character and a hidden state (previous output before being passed to the softmax)--> this is why in the architecture we have to return the hidden state\n","- it will serve as the output at the next iteration \n","- Hidden state has always the same shape since it is not passed to the softmax\n","- At every stage there is an output\n","\n","\n","### Open Question:\n","Explain the dimensions of the hidden state\n","why are they different from the output?\n","Does \"creating\" new hidden/cell state mean that we only utilize the information for one batch ?"]},{"cell_type":"code","metadata":{"id":"dO_49_t1-yIM","colab_type":"code","colab":{}},"source":["class LSTM(nn.Module):\n","    \"\"\"Tokens is the number unique characters in the corpus\"\"\"\n","    \n","    def __init__(self,\n","                 tokens,\n","                 hidden_size=512,\n","                 num_layers=2,\n","                 dropout=0.2,\n","                 lr=0.001\n","                \n","                ):\n","        super().__init__()\n","        \n","        self.chars=tokens\n","        self.input_size=len(tokens)\n","        self.hidden_size=hidden_size\n","        self.num_layers=num_layers\n","        self.dropout=dropout\n","        \n","        \n","        self.int2char = dict(enumerate(self.chars))\n","        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","        \n","        self.lstm=nn.LSTM(self.input_size,\n","                          self.hidden_size,\n","                          self.num_layers,\n","                          dropout=self.dropout,\n","                          batch_first=True)\n","        self.dropout=nn.Dropout(p=0.2)\n","        self.fc1=nn.Linear(self.hidden_size, self.input_size)\n","        \n","        \n","    def forward(self,x, hidden):\n","      x,self.hidden=self.lstm(x,hidden)\n","\n","      # shape of x is (batchsize,seq lenght and 2)--> why two?\n","      # \n","\n","#         output has the same shape as hidden\n","#         since the next layer is a linear fully connected layer the input has to be an array\n","#         therefore the output has to be stacked\n","      x=self.dropout(x)\n","      x=x.contiguous().view(-1, self.hidden_size)\n","      output=self.fc1(x)\n","        \n","      return output, hidden\n","  \n","#     This comes straight from the pytorch documentation and seems to be pretty standard\n","    def init_hidden(self, batch_size):\n","      weight=next(self.parameters()).data\n","      if (train_on_gpu):\n","        # hidden has both the long and short term memory\n","        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().cuda(),\n","                  weight.new(self.num_layers, batch_size, self.hidden_size).zero_().cuda())\n","      else:\n","          hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n","                    weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n","        \n","      return hidden\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jpatIZnaCnio","colab_type":"text"},"source":["Creating a function that saves the model parameters--> I will need this for the optional parts"]},{"cell_type":"code","metadata":{"id":"etqXrweDCV7J","colab_type":"code","colab":{}},"source":["def save_model(current_epoch, model,batch_size, seq_length):\n","  model_name = 'rnn_' + str(current_epoch) + '_epoch.net'\n","  checkpoint = {'hidden_size': model.hidden_size,\n","              'n_layers': model.num_layers,\n","              'batch_size': batch_size,\n","              'sequence_length': seq_length,\n","              'state_dict': model.state_dict(),\n","              'chars': model.chars}\n"," \n","  with open(model_name, 'wb') as f:\n","    torch.save(checkpoint, f)\n","    \n","  print('successfully saved model')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sVvvyRFR-yIP","colab_type":"text"},"source":["Next we train the network--> lookout for the cellstate hidden state variables. "]},{"cell_type":"code","metadata":{"id":"ottEbnbsFzDU","colab_type":"code","colab":{}},"source":["def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=100, save=False):\n","    val_loss_tracker = []\n","\n","    net.train()\n","    \n","    optimizer=optim.Adam(net.parameters(), lr=lr)\n","#     not having used softmax on the output we can use crossentropyloss here \n","    criterion=nn.CrossEntropyLoss()\n","    \n","    train_size=int(len(data)*(1-val_frac))\n","    train_data,vali_data=data[:train_size],data[train_size:]\n","    \n","    \n","    if(train_on_gpu):\n","        net.cuda()\n","        \n","    counter=0\n","    n_chars=len(net.chars) #how many different characters are there\n","    \n","    for e in range(epochs):\n","        hidden=net.init_hidden(batch_size)\n","        \n","#         we first get the batches of the characters\n","#         then we one hot encode them\n","        for x,y in get_batches(train_data, batch_size, seq_length):\n","          counter+=1\n","            \n","          x=one_hot_encode(x,n_chars)\n","          inputs, targets= torch.from_numpy(x), torch.from_numpy(y)\n","            \n","          if(train_on_gpu):\n","            inputs, targets = inputs.cuda(), targets.cuda()\n","#           detaching hidden state\n","          hidden=tuple([var.data for var in hidden])\n","    \n","          net.zero_grad()\n","        \n","\n","#         here the entire sequence is processed in one go--> I believe\n","          output,hidden=net(inputs, hidden)\n","        \n","          loss=criterion(output, targets.view(batch_size*seq_length))\n","          loss.backward()\n","            \n","          nn.utils.clip_grad_norm_(net.parameters(),clip)\n","          optimizer.step()\n","        else:\n","          if save:\n","            save_model(e, net, seq_length, batch_size)\n","\n","          hidden_val = net.init_hidden(batch_size)\n","          val_losses = []\n","          net.eval()\n","\n","          for x,y in get_batches(vali_data, batch_size, seq_length):\n","\n","            x=one_hot_encode(x,n_chars)\n","            inputs, targets= torch.from_numpy(x), torch.from_numpy(y)\n","            hidden_val=tuple([var.data for var in hidden_val])\n","                # send data to GPU\n","            if(train_on_gpu):\n","              inputs, targets = inputs.cuda(), targets.cuda()\n","\n","            output,hidden_val=net(inputs, hidden_val)    \n","            val_loss=criterion(output, targets.view(batch_size*seq_length))\n","            val_losses.append(val_loss.item())\n","\n","          net.train()\n","          \n","          print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                \"Step: {}...\".format(counter),\n","                \"Loss: {:.4f}...\".format(loss.item()),\n","                \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n","          val_loss_tracker.append(np.mean(val_losses))\n","          if len(val_loss_tracker)>2 and val_loss_tracker[-1]>(val_loss_tracker[-2]+val_loss_tracker[-3])/2:\n","            print('Stopping due to early stopping criterion')\n","            print(f'load model rnn_{str(e)}_epoch.net')\n","            return e\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyPCmm2Y-yIS","colab_type":"code","outputId":"58918456-72c2-436d-996e-5b169bf0ebe1","executionInfo":{"status":"ok","timestamp":1587649264052,"user_tz":-120,"elapsed":1018,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["#setting a few hyperparamters\n","n_hidden=1024\n","n_layers=3\n","net=LSTM(chars,n_hidden,n_layers)\n","print(net)"],"execution_count":102,"outputs":[{"output_type":"stream","text":["LSTM(\n","  (lstm): LSTM(83, 1024, num_layers=3, batch_first=True, dropout=0.2)\n","  (dropout): Dropout(p=0.2, inplace=False)\n","  (fc1): Linear(in_features=1024, out_features=83, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sgkgbMTU-yIV","colab_type":"code","outputId":"f3535898-3904-489c-b771-c7a87676984c","executionInfo":{"status":"ok","timestamp":1587650530295,"user_tz":-120,"elapsed":1224382,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}},"colab":{"base_uri":"https://localhost:8080/","height":561}},"source":["n_epochs=10\n","batch_size=128\n","seq_length=100\n","model_to_loadtrain=train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=100, save=True)\n"],"execution_count":105,"outputs":[{"output_type":"stream","text":["successfully saved model\n","Epoch: 1/80... Step: 139... Loss: 3.0881... Val Loss: 3.0643\n","successfully saved model\n","Epoch: 2/80... Step: 278... Loss: 2.1528... Val Loss: 2.1658\n","successfully saved model\n","Epoch: 3/80... Step: 417... Loss: 1.7579... Val Loss: 1.7662\n","successfully saved model\n","Epoch: 4/80... Step: 556... Loss: 1.5607... Val Loss: 1.5763\n","successfully saved model\n","Epoch: 5/80... Step: 695... Loss: 1.4504... Val Loss: 1.4764\n","successfully saved model\n","Epoch: 6/80... Step: 834... Loss: 1.3834... Val Loss: 1.4193\n","successfully saved model\n","Epoch: 7/80... Step: 973... Loss: 1.3270... Val Loss: 1.3781\n","successfully saved model\n","Epoch: 8/80... Step: 1112... Loss: 1.2953... Val Loss: 1.3498\n","successfully saved model\n","Epoch: 9/80... Step: 1251... Loss: 1.2633... Val Loss: 1.3275\n","successfully saved model\n","Epoch: 10/80... Step: 1390... Loss: 1.2396... Val Loss: 1.3181\n","successfully saved model\n","Epoch: 11/80... Step: 1529... Loss: 1.2149... Val Loss: 1.3065\n","successfully saved model\n","Epoch: 12/80... Step: 1668... Loss: 1.1929... Val Loss: 1.2959\n","successfully saved model\n","Epoch: 13/80... Step: 1807... Loss: 1.1721... Val Loss: 1.2952\n","successfully saved model\n","Epoch: 14/80... Step: 1946... Loss: 1.1503... Val Loss: 1.2946\n","successfully saved model\n","Epoch: 15/80... Step: 2085... Loss: 1.1312... Val Loss: 1.2989\n","Stopping due to early stopping criterion\n","load model rnn_14_epoch.net\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XHCOTFVvHY6P","colab_type":"text"},"source":["## Predicting new characters\n","\n","We Take all possible characters in and start with a hidden state of None\n","Then we predict the most likely character\n","At the next step, we predict the most likely character given the one before/\n"]},{"cell_type":"code","metadata":{"id":"a4x6Edz6HanO","colab_type":"code","colab":{}},"source":["def predict_char(net,char,hidden=None,top_k=None):\n","  # character gets encoded with its numerical value\n","  # next we create the one vector for this one char\n","  inputs=np.array([[net.char2int[char]]])\n","  inputs=one_hot_encode(inputs, len(net.chars))\n","  inputs=torch.from_numpy(inputs)\n","  if train_on_gpu:\n","    inputs=inputs.cuda()\n","  \n","  hidden=tuple([var.data for var in hidden])\n","\n","  output,hidden=net(inputs,hidden)\n","  probability=F.softmax(output,dim=1).data\n","  if(train_on_gpu):\n","    probability = probability.cpu() \n","  if top_k is None:\n","    # getting the top probabilities and the top characters not all of them\n","    top_ch=np.arange(len(net.chars))\n","  else:\n","    probability,top_ch=probability.topk(top_k)\n","    # turning tensor into numpy array\n","    top_ch=top_ch.numpy().squeeze()\n","\n","  probability=probability.numpy().squeeze()\n","  char=np.random.choice(top_ch, p=probability/probability.sum())\n","    # p is the rescalded proability --> unchanged if top_k=None\n","  return net.int2char[char], hidden"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dzCU-JyBJ7nU","colab_type":"code","colab":{}},"source":["def sample(net, size, prime='The', top_k=None):\n","  if(train_on_gpu):\n","    net.cuda()\n","  else:\n","    net.cpu()\n","  net.eval() # eval mode\n","\n","  chars = [ch for ch in prime]\n","  hidden = net.init_hidden(1)\n","  for ch in prime:\n","    char, hidden = predict_char(net, ch, hidden, top_k=top_k)\n","\n","  chars.append(char)\n","\n","  for ii in range(size):\n","    char, hidden = predict_char(net, chars[-1], hidden, top_k=top_k)\n","    chars.append(char)\n","\n","  return ''.join(chars)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8obCT5FMufA","colab_type":"code","outputId":"32737d77-7827-427e-fa1b-4f034d7d8e9b","executionInfo":{"status":"ok","timestamp":1587650574301,"user_tz":-120,"elapsed":1960,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"source":["print(sample(net, 1000, prime='Anna', top_k=10))"],"execution_count":108,"outputs":[{"output_type":"stream","text":["Annan, tethed, s in ting,\"Saledrataphanssored.\n","Aleas witeevillofore andove hethenends saig sasoveshe ashed f otingeshin heryorerase t t whig crid foon as t br,\"Wheay hes s aserean omo owat sitha thit thest omelasevetond ofand sht he ten co he wing an tedirorain iteved aimpad tonot sinsh, as fa f t she hay..\n","ounean frirono cait as ad\n","\n","aite can wing tone ovede t hend ttath t he t womer feleret fallerouns and as atanofon in to helly t heron comiot ofofonorthay tererimaimen ourse thaidontevid tisthindovendang\n","tirowheve ane aind alyedisind s a ont h a stheen sthas thinghathisa s hant, arely, het fasiotindithimyore ced thof thed bled she htt toug, war sass wit alles bofatheseland, bund funtiof iof orond ion hiouped in t tas th hindoonoumotathan ore bure othowist iout he s ape f chtin say sus seranot allye herovowe s f berd inthe outy oro hitin itofediche the ce aliryoutinge athaine at heantord fulealon f a helighe an at hrad heralor ss benetot ano thiritone t ay thedeareell ste be we wal t he atr\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ykdTWcNJB6gs","colab_type":"text"},"source":["### Loading the best model\n","\n"]},{"cell_type":"code","metadata":{"id":"dgqopYxAAD5k","colab_type":"code","colab":{}},"source":["with open(f'rnn_{model_to_load}_epoch.net', 'rb') as f:\n","    checkpoint = torch.load(f)\n"," \n","loaded = CharLSTM_extended(batch_size=checkpoint['batch_size'],seq_length=checkpoint['seq_length'], \n","                           chars=checkpoint['chars'],\n","                           hidden_dim=checkpoint['hidden_dim'], \n","                           n_layers=checkpoint['n_layers'])\n","loaded.load_state_dict(checkpoint['state_dict'])"],"execution_count":0,"outputs":[]}]}