{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"spark","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Homework_III_FC.ipynb","provenance":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"DrsBVDJc6GNc","colab_type":"text"},"source":["\n","# Homework III: Character-Level Language Model with LSTMs in PyTorch\n","\n","\n","\n","------------------------------------------------------\n","**Machine Learning. Master in Big Data Analytics**\n","\n","*Pablo M. Olmos pamartin@ing.uc3m.es*\n","\n","------------------------------------------------------\n","\n","\n","In this notebook, construct a character-level LSTM with PyTorch. The network will train character by character on some text, then generate new text character by character. As an example, I will train on Anna Karenina. **This model will be able to generate new text based on the text from the book!**\n","\n","This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Below is the general architecture of the character-wise RNN.\n"]},{"cell_type":"code","metadata":{"id":"0qBCh0G56GNd","colab_type":"code","outputId":"efa9d2ae-1e4d-46d6-c873-aa389225fb4c","executionInfo":{"status":"ok","timestamp":1587643341602,"user_tz":-120,"elapsed":1063,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}},"colab":{"base_uri":"https://localhost:8080/","height":221}},"source":["from IPython.display import Image\n","from IPython.core.display import HTML \n","\n","Image(url= \"http://karpathy.github.io/assets/rnn/charseq.jpeg\", width=500, height=200)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"http://karpathy.github.io/assets/rnn/charseq.jpeg\" width=\"500\" height=\"200\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"ZjMl9nMb6GNi","colab_type":"text"},"source":["First let's load in our required resources for data loading and model creation."]},{"cell_type":"code","metadata":{"id":"ofXk1CcU6GNi","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","from torch import optim\n","import numpy as np\n","import time\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q2I7QmR-6GNl","colab_type":"text"},"source":["## Part I. Load in Data\n","\n","First, we load the Anna Karenina text file and convert it into integers for our network to use. "]},{"cell_type":"code","metadata":{"id":"9eskxeaI6v3N","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":71},"outputId":"cf97afed-5166-4b43-8d38-0a69b3070c36","executionInfo":{"status":"ok","timestamp":1587643543569,"user_tz":-120,"elapsed":58033,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}}},"source":["import torch\n","from torch import nn\n","import numpy as np\n","import torch.nn.functional as F\n","from torch import optim\n","\n","from google.colab import files\n","text=files.upload()"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-a1c2acfd-52aa-415d-9cc1-cf2c04da2320\" name=\"files[]\" multiple disabled />\n","     <output id=\"result-a1c2acfd-52aa-415d-9cc1-cf2c04da2320\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving anna.txt to anna.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"uZzCKDSA6GNl","colab_type":"code","colab":{}},"source":["# open text file and read in data as `text`\n","with open('anna.txt', 'r') as f:\n","    text = f.read()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1slyVd-J6GNo","colab_type":"text"},"source":["Let's check out the first 100 characters, make sure everything is peachy. According to the [American Book Review](http://americanbookreview.org/100bestlines.asp), this is the 6th best first line of a book ever."]},{"cell_type":"code","metadata":{"id":"EkXDV-uk6GNo","colab_type":"code","outputId":"944fbc4f-f3df-4ebf-e7d5-e376788968fb","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1587643550482,"user_tz":-120,"elapsed":1133,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}}},"source":["text[:100]"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"t_iGhECk6GNu","colab_type":"text"},"source":["### Tokenization\n","\n","In the cells, below, I'm creating a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network.\n","\n","Your can read more about Python **set data types** in this [link](https://docs.python.org/3.8/library/stdtypes.html#set-types-set-frozenset). Also, check [here](http://book.pythontips.com/en/latest/enumerate.html) about the `enumerate()` Python built-in function."]},{"cell_type":"code","metadata":{"id":"d3MPIhwS6GNv","colab_type":"code","colab":{}},"source":["# encode the text and map each character to an integer and vice versa\n","\n","# we create two dictionaries:\n","# 1. int2char, which maps integers to characters\n","# 2. char2int, which maps characters to unique integers\n","#set() is an unordered collection of unique elements in text\n","chars = tuple(set(text))\n","int2char = dict(enumerate(chars))\n","char2int = {ch: ii for ii, ch in int2char.items()}\n","\n","# encode the text\n","encoded = np.array([char2int[ch] for ch in text])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zgzVSVal6GNy","colab_type":"text"},"source":["And we can see those same characters from above, encoded as integers."]},{"cell_type":"code","metadata":{"id":"9vDRW8ZY6GNy","colab_type":"code","outputId":"079f4b02-0611-4a82-a29a-f6511c4afaf1","colab":{}},"source":["encoded[:100]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([25, 81, 78, 77, 75, 23, 27, 28,  6,  3,  3,  3, 26, 78, 77, 77, 29,\n","       28, 79, 78, 42,  4,  0,  4, 23, 41, 28, 78, 27, 23, 28, 78,  0,  0,\n","       28, 78,  0,  4, 55, 23, 59, 28, 23, 30, 23, 27, 29, 28,  8, 71, 81,\n","       78, 77, 77, 29, 28, 79, 78, 42,  4,  0, 29, 28,  4, 41, 28,  8, 71,\n","       81, 78, 77, 77, 29, 28,  4, 71, 28,  4, 75, 41, 28, 74, 14, 71,  3,\n","       14, 78, 29, 61,  3,  3, 72, 30, 23, 27, 29, 75, 81,  4, 71])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"FF47SyyV6GN2","colab_type":"text"},"source":["## Part II. Pre-processing the data\n","\n","As you can see in our char-RNN image above, our LSTM expects an input that is **one-hot encoded** meaning that each character is converted into an integer (via our created dictionary) and *then* converted into a column vector where only it's corresponding integer index will have the value of 1 and the rest of the vector will be filled with 0's. Since we're one-hot encoding the data, let's make a function to do that!\n"]},{"cell_type":"code","metadata":{"id":"Ye1aOTa-6GN2","colab_type":"code","colab":{}},"source":["# arr is an encoded text array (every row is a different text line)\n","\n","def one_hot_encode(arr, n_labels):\n","    \n","    num_elements = arr.shape[0]*arr.shape[1]\n","    \n","    # Initialize the the encoded array\n","    one_hot = np.zeros([num_elements, n_labels], dtype=np.float32)\n","    \n","    # Fill the appropriate elements with ones\n","    one_hot[np.arange(one_hot.shape[0]), arr.reshape([num_elements])] = 1.\n","    \n","    # Reshape as (n_batch,seq_length,n_labels)\n","    \n","    return one_hot.reshape(arr.shape[0],arr.shape[1],n_labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_O7yHqA6GN4","colab_type":"code","outputId":"e4398926-e673-4438-afb5-e1d8ccb7b3b9","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1587643580447,"user_tz":-120,"elapsed":1003,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}}},"source":["# check that the function works as expected\n","test_seq = np.array([[3, 5, 1],[2,4,6]])\n","one_hot = one_hot_encode(test_seq, 8)\n","\n","print(one_hot)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[[0. 0. 0. 1. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 1. 0. 0.]\n","  [0. 1. 0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 1. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 1. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0. 1. 0.]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-p4qKSuf6GN7","colab_type":"text"},"source":["## Part III. Making training mini-batches\n","\n","\n","To train on this data, we also want to create mini-batches for training. Remember that we want our batches to be multiple sequences of some desired number of sequence steps. Considering a simple example, our batches would look like this."]},{"cell_type":"code","metadata":{"id":"Tfmi8YjV6GN7","colab_type":"code","outputId":"0e77709a-3aed-44d4-ff03-f964d9e84b62","colab":{}},"source":["Image(url= \"http://digtime.cn/uploads/images/201907/12/1/Tt94B4Htu7.png\", width=700, height=200)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"http://digtime.cn/uploads/images/201907/12/1/Tt94B4Htu7.png\" width=\"700\" height=\"200\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"GFfU9fxg6GN9","colab_type":"text"},"source":["In this example, we'll take the encoded characters (passed in as the `arr` parameter) and split them into multiple sequences, given by `batch_size`. Each of our sequences will be `seq_length` long.\n","\n","### Creating Batches\n","\n","**1. The first thing we need to do is discard some of the text so we only have completely full mini-batches. **\n","\n","Each batch contains $N \\times M$ characters, where **$N$ is the batch size** (the number of sequences in a batch) and $M$ is the seq_length or number of time steps in a sequence. Then, to get the total number of batches, $K$, that we can make from the array `arr`, you divide the length of `arr` by the number of characters per batch. Once you know the number of batches, you can get the total number of characters to keep from `arr`, $N * M * K$.\n","\n","**2. After that, we need to split `arr` into batches. ** \n","\n","You can do this using `arr.reshape(size)` where `size` is a tuple containing the dimensions sizes of the reshaped array. We know we want $N$ sequences in a batch, so let's make that the size of the first dimension. For the second dimension, you can use `-1` as a placeholder in the size, it'll fill up the array with the appropriate data for you. After this, you should have an array that is $N \\times (M * K)$.\n","\n","**3. Now that we have this array, we can iterate through it to get our mini-batches. **\n","\n","The idea is each batch is a $N \\times M$ window on the $N \\times (M * K)$ array. For each subsequent batch, the window moves over by `seq_length`. We also want to create both the input and target arrays. Remember that the targets are just the inputs shifted over by one character. \n","\n","> **Exercise:** Understand the following code for creating batches in the function below. **Note:** Take a look to this [link](https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do) to understand what `yield` does."]},{"cell_type":"code","metadata":{"id":"XS7VFnao6GOA","colab_type":"code","colab":{}},"source":["def get_batches(arr, batch_size, seq_length):\n","    '''Create a generator that returns batches of size\n","       batch_size x seq_length from arr.\n","       \n","       Arguments\n","       ---------\n","       arr: Array you want to make batches from\n","       batch_size: Batch size, the number of sequences per batch\n","       seq_length: Number of encoded chars in a sequence\n","    '''\n","    \n","    batch_size_total = batch_size * seq_length\n","    # total number of batches we can make\n","    n_batches = len(arr)//batch_size_total\n","    \n","    # Keep only enough characters to make full batches\n","    arr = arr[:n_batches * batch_size_total]\n","    # Reshape into batch_size rows\n","    arr = arr.reshape((batch_size, -1))      # IMPORTANT TO UNDERSTAND THE DIMENSIONS OF arr\n","    \n","    # iterate through the array. We read every of the n_batches sequeces in windows of seq_length characters\n","    for n in range(0, arr.shape[1], seq_length):\n","        if (n+seq_length<arr.shape[1]):\n","            # The features\n","            x = arr[:, n:n+seq_length]\n","            # The targets, shifted by one\n","            y = np.zeros_like(x)\n","            y[:,-1] = arr[:, n+seq_length]\n","            y[:, :-1] = x[:, 1:]\n","        else: \n","            # If n+seq_length == arr.shape[1], then we are done! (arr[:, n+seq_length] yields an indexing error).\n","            # We return a window with one column less (as the target for the last character is not available)\n","            x = arr[:, n:n+seq_length-1]\n","            y = np.zeros_like(x)\n","            y[:, :-1] = x[:, 1:]\n","            y[:,-1] = arr[:,-1]            \n","        yield x, y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2oapUpE6GOD","colab_type":"text"},"source":["### Test Your Implementation\n","\n","Lets make some data sets and we can check out what's going on as we batch data. Here, as an example, I'm going to use a batch size of 8 and 50 sequence steps."]},{"cell_type":"code","metadata":{"id":"2hnAtgjY6GOE","colab_type":"code","outputId":"2544cb45-a3d5-46dd-8c5c-972d918cd5fe","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1587643603742,"user_tz":-120,"elapsed":754,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}}},"source":["batches = get_batches(encoded, 8, 50)\n","x,y = next(batches)\n","print(x.shape)\n","print(y.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["(8, 50)\n","(8, 50)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"scrolled":true,"id":"Xz9Ai1CM6GOG","colab_type":"code","outputId":"d2ab96ea-7fa5-4eb2-992e-4bbdbaa63643","colab":{"base_uri":"https://localhost:8080/","height":340},"executionInfo":{"status":"ok","timestamp":1587643605240,"user_tz":-120,"elapsed":966,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}}},"source":["# printing out the first 10 items in a sequence\n","print('x\\n', x[:10, :10])\n","print('\\ny\\n', y[:10, :10])"],"execution_count":13,"outputs":[{"output_type":"stream","text":["x\n"," [[36 59 61 81 54 51 60  1 64 70]\n"," [80 17 13  1 54 59 61 54  1 61]\n"," [51 13  3  1 17 60  1 61  1 16]\n"," [80  1 54 59 51  1 32 59  4 51]\n"," [ 1 80 61  7  1 59 51 60  1 54]\n"," [32 39 80 80  4 17 13  1 61 13]\n"," [ 1 73 13 13 61  1 59 61  3  1]\n"," [24  2 69 17 13 80 52 55 72  1]]\n","\n","y\n"," [[59 61 81 54 51 60  1 64 70 70]\n"," [17 13  1 54 59 61 54  1 61 54]\n"," [13  3  1 17 60  1 61  1 16 17]\n"," [ 1 54 59 51  1 32 59  4 51 16]\n"," [80 61  7  1 59 51 60  1 54 51]\n"," [39 80 80  4 17 13  1 61 13  3]\n"," [73 13 13 61  1 59 61  3  1 80]\n"," [ 2 69 17 13 80 52 55 72  1 45]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P0P22fXG6GOI","colab_type":"text"},"source":["The above output should look something like \n","```\n","x\n"," [[25  8 60 11 45 27 28 73  1  2]\n"," [17  7 20 73 45  8 60 45 73 60]\n"," [27 20 80 73  7 28 73 60 73 65]\n"," [17 73 45  8 27 73 66  8 46 27]\n"," [73 17 60 12 73  8 27 28 73 45]\n"," [66 64 17 17 46  7 20 73 60 20]\n"," [73 76 20 20 60 73  8 60 80 73]\n"," [47 35 43  7 20 17 24 50 37 73]]\n","\n","y\n"," [[ 8 60 11 45 27 28 73  1  2  2]\n"," [ 7 20 73 45  8 60 45 73 60 45]\n"," [20 80 73  7 28 73 60 73 65  7]\n"," [73 45  8 27 73 66  8 46 27 65]\n"," [17 60 12 73  8 27 28 73 45 27]\n"," [64 17 17 46  7 20 73 60 20 80]\n"," [76 20 20 60 73  8 60 80 73 17]\n"," [35 43  7 20 17 24 50 37 73 36]]\n"," ```\n"," although the exact numbers may be different. Check to make sure the data is shifted over one step for `y`."]},{"cell_type":"markdown","metadata":{"id":"Aot-zs5T6GOI","colab_type":"text"},"source":["---\n","## Part IV. Defining the network with PyTorch\n","\n","Below is where you'll define the network."]},{"cell_type":"code","metadata":{"id":"doqKo8f96GOJ","colab_type":"code","outputId":"53ce79a7-8b06-451c-86b9-81ce2a8e2607","colab":{"base_uri":"https://localhost:8080/","height":221},"executionInfo":{"status":"ok","timestamp":1587643608728,"user_tz":-120,"elapsed":1026,"user":{"displayName":"Fred Che","photoUrl":"","userId":"07612464138287230381"}}},"source":["Image(url= \"http://digtime.cn/uploads/images/201907/12/1/ZG9JIjidmz.png\", width=500, height=200)"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<img src=\"http://digtime.cn/uploads/images/201907/12/1/ZG9JIjidmz.png\" width=\"500\" height=\"200\"/>"],"text/plain":["<IPython.core.display.Image object>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"-TaynxT_6GOL","colab_type":"text"},"source":["#### Model Structure\n","\n","In `__init__` the suggested structure is as follows:\n","* Create and store the necessary dictionaries \n","* Define an LSTM layer that takes as params: an input size (the number of characters), a hidden layer size `n_hidden`, a number of layers `n_layers`, a dropout probability `drop_prob`, and a batch_first boolean (True, since we are batching)\n","* Define a dropout layer with `drop_prob`\n","* Define a fully-connected layer with params: input size `n_hidden` and output size (the number of characters)\n","* Finally, initialize LSTM hidden state (h_0) and memory (c_0)\n","\n","\n","#### LSTM Inputs/Outputs\n","\n","You can create a basic [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) as follows\n","\n","```python\n","self.lstm = nn.LSTM(input_size, n_hidden, n_layers, \n","                            dropout=drop_prob, batch_first=True)\n","```\n","\n","where `input_size` is the number of characters this cell expects to see as sequential input, and `n_hidden` is the number of units in the hidden layers in the cell. And we can add dropout by adding a dropout parameter with a specified probability; this will automatically add dropout to the inputs or outputs. \n","\n","\n","> **Exercise:** Complete the following code\n"]},{"cell_type":"code","metadata":{"id":"VcdWmmka6GOL","colab_type":"code","colab":{}},"source":["class CharLSTM(nn.Module):\n","    \n","    def __init__(self, n_chars, hidden_dim=256, n_layers=2,drop_prob=0.5):\n","        \n","        ''' n_chars is the number of different characters '''        \n","        \n","        super().__init__()\n","        self.drop_prob = drop_prob\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim        \n","        \n","        ## TODO: define the LSTM. Do not forget to include the dropout between LSTM layers!\n","        self.lstm=nn.LSTM(self.input_size,\n","                          self.hidden_size,\n","                          self.num_layers,\n","                          dropout=self.drop_out,\n","                          batch_first=True)\n","        \n","        self.dropout = nn.Dropout(drop_prob)\n","        \n","        ## TODO: define the final, fully-connected output layer\n","        self.fc1=nn.Linear(self.hidden_size, self.input_size)\n","        \n","        self.logsoftmax = nn.LogSoftmax(dim=1) \n","      \n","    \n","    def forward(self, x, h=None):\n","        ''' Forward pass through the network. \n","            These inputs are x, and the LSTM initial hidden/cell state h. \n","            When h is not provided, default initializaiton (all zeros) is used.'''\n","        \n","        if (h==None):\n","            #No initial hidden_state and memory are provided. They are set to 0s\n","            r_output, hidden = self.lstm(x)  \n","        \n","        else:\n","            #Non-zero initial state. This is used for generating new text\n","            r_output, hidden = self.lstm(x,h)     \n","        \n","        ## TODO: pass through a dropout layer\n","        out = self.dropout(r_output)\n","        \n","        # Stack up LSTM outputs\n","        out = out.reshape(-1, self.hidden_dim) \n","        \n","        ## TODO: put x through the fully-connected layer and a logsoftmax output\n","        out = self.fc1(out)\n","        out = self.logsoftmax(out)\n","        \n","        # return the final output and the hidden state\n","        return out, hidden\n","    \n","\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"emfF3nko6GOO","colab_type":"text"},"source":["The following code extends the above class by including a training method, and we also incorporate the methods that we implemented above  to perform one-hot encoding and get the mini-batches from the text. One detail about training: we use [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html) to help prevent exploding gradients.\n","\n","> **Exercise:** Complete the following code\n"]},{"cell_type":"code","metadata":{"id":"mopXfIWP6GOO","colab_type":"code","colab":{}},"source":["class CharLSTM_extended(CharLSTM):\n","    \n","    \n","    def __init__(self, batch_size, sequence_length, chars, hidden_dim=256, n_layers=2, \n","                 epochs=5, drop_prob=0.5, lr=0.001,clip=5):\n","        \n","        self.n_char = len(chars)\n","            \n","        super().__init__(self.n_char, hidden_dim, n_layers,drop_prob)  \n","        \n","        self.batch_size = batch_size\n","        \n","        self.clip = clip\n","        \n","        self.seq_length = sequence_length\n","        \n","        self.num_layers = n_layers\n","        \n","        self.lr = lr #Learning Rate\n","    \n","        self.optim = optim.Adam(self.parameters(), self.lr)\n","        \n","        self.epochs = epochs\n","        \n","        self.criterion = nn.NLLLoss()             \n","        \n","        # A list to store the loss evolution along training\n","        \n","        self.loss_during_training = [] \n","        \n","        # Setting GPU training if available\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","        self.to(self.device)\n","        \n","        # Training mode by default\n","        self.train()\n","        \n","    def get_batches(self,arr):\n","        '''Create a generator that returns batches of size\n","           batch_size x seq_length from arr.\n","\n","           Arguments\n","           ---------\n","           arr: Array you want to make batches from\n","           batch_size: Batch size, the number of sequences per batch\n","           seq_length: Number of encoded chars in a sequence\n","        '''\n","        \n","        batch_size_total = self.batch_size * self.seq_length\n","        # total number of batches we can make\n","        n_batches = len(arr)//batch_size_total\n","\n","        # Keep only enough characters to make full batches\n","        arr = arr[:n_batches * batch_size_total]\n","        # Reshape into batch_size rows\n","        arr = arr.reshape((self.batch_size, -1))      # IMPORTANT TO UNDERSTAND THE DIMENSIONS OF arr\n","\n","        # iterate through the array. We read every of the n_batches sequeces in windows of seq_length characters\n","        for n in range(0, arr.shape[1], self.seq_length):\n","\n","\n","            if (n+self.seq_length<arr.shape[1]):\n","                # The features\n","                x = arr[:, n:n+self.seq_length]\n","                # The targets, shifted by one\n","                y = np.zeros_like(x)\n","                y[:,-1] = arr[:, n+self.seq_length]\n","                y[:, :-1] = x[:, 1:]\n","                \n","            else: \n","                # If n+seq_length == arr.shape[1], then we are done! (arr[:, n+seq_length] yields an indexing error).\n","                # We return a window with one column less (as the target for the last character is not available)\n","                x = arr[:, n:n+self.seq_length-1]\n","                y = np.zeros_like(x)\n","                y[:, :-1] = x[:, 1:]\n","                y[:,-1] = arr[:,-1]  \n","                \n","            yield x, y\n","            \n","    def one_hot_encode(self,arr):\n","    \n","        #YOUR CODE HERE (multiple lines here!)\n","    \n","    def trainloop(self,encoded_text):\n","        \n","        for e in range(self.epochs):\n","            \n","            counter = 0.\n","            \n","            start_time = time.time()\n","            \n","            running_loss = 0.\n","            \n","            for x, y in self.get_batches(encoded_text):\n","                \n","                counter += 1.\n","                \n","                # One-hot encode our data and make them Torch tensors\n","                x = self.one_hot_encode(x)\n","\n","                x, y = torch.from_numpy(x).to(self.device), torch.from_numpy(y).to(self.device)\n","                \n","                # TO DO: reset gradients \n","                #YOUR CODE HERE\n","                \n","                # TO DO: compute output\n","                #YOUR CODE HERE\n","                \n","                # TO DO: calculate the loss and perform backprop\n","                loss = self.criterion(out, y.reshape([out.shape[0],])) #YOUR CODE HERE\n","                \n","                # TO DO: compute gradients\n","                #YOUR CODE HERE\n","                \n","                # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","                nn.utils.clip_grad_norm_(self.parameters(), self.clip)\n","                \n","                self.optim.step() \n","                \n","                running_loss += loss.item()\n","                \n","            self.loss_during_training.append(running_loss/counter)\n","            \n","            if(e % 1 == 0): \n","                \n","                print(\"Epoch %d. Training loss: %f, Time per epoch: %f seconds\" \n","                      %(e,self.loss_during_training[-1],(time.time() - start_time)))            \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6QsPmdlM6GOS","colab_type":"text"},"source":["Finallly, we create an object of type `CharLSTM_extended` and train for a few epochs. It takes a while though (training in Google Colab is advised) ..."]},{"cell_type":"code","metadata":{"id":"qaBjeDZy6GOT","colab_type":"code","colab":{}},"source":["my_charLSTM = CharLSTM_extended(batch_size=128,sequence_length=100,chars=chars,\n","                                epochs=10,hidden_dim=512,n_layers=2,drop_prob=0.3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvhRmGcv6GOV","colab_type":"code","outputId":"a28be9f9-9c8b-4450-8f03-2cdae382e6d2","colab":{}},"source":["my_charLSTM.trainloop(encoded)"],"execution_count":0,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-546abc924649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_charLSTM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainloop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-24-7a0862f1168c>\u001b[0m in \u001b[0;36mtrainloop\u001b[0;34m(self, encoded_text)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0;31m# TO DO: compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 \u001b[0;31m# `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/work/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/envs/work/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"eWXX5ZK96GOX","colab_type":"text"},"source":["---\n","## Part VI. Making Predictions\n","\n","Now that the model is trained, we'll want to sample from it and make predictions about next characters! To sample, we pass in a character and have the network predict the next character. Then we take that character, pass it back in, and get another predicted character. Just keep doing this and you'll generate a bunch of text!\n","\n","### A note on the `predict`  function\n","\n","The output of our RNN is from a fully-connected layer followed by a Softmax layer, and it outputs a **distribution of next-character scores**.\n","\n","### Top K sampling\n","\n","Our predictions come from a categorical probability distribution over all the possible characters. We can make the sample text and make it more reasonable to handle (with less variables) by only considering some $K$ most probable characters. This will prevent the network from giving us completely absurd characters while allowing it to introduce some noise and randomness into the sampled text. Read more about [topk, here](https://pytorch.org/docs/stable/torch.html#torch.topk).\n","\n","> **Exercise:** Complete the following code"]},{"cell_type":"code","metadata":{"id":"dzf8p-Mu6GOY","colab_type":"code","colab":{}},"source":["def predict(charLSTM_class, current_char, dict_c2int,h=None, top_k=None):\n","        ''' Given a character, predict the next character.\n","            Returns the predicted character and the hidden state.\n","        '''\n","        \n","        # tensor inputs\n","        x = np.array([[char2int[current_char]]])\n","        x = charLSTM_class.one_hot_encode(x)\n","        \n","        inputs = torch.from_numpy(x)\n","        \n","        charLSTM_class.to('cpu')\n","        \n","        # TO DO: get the output of the model\n","        out, h = charLSTM_class.forward(inputs, h) #YOUR CODE HERE\n","\n","        p, top_ch = torch.exp(out).topk(top_k)     #Note that we exponentiate to get actual probabilities!\n","        top_ch = top_ch.numpy().squeeze()\n","        \n","        # select the likely next character with some element of randomness\n","        p = p.detach().numpy().squeeze()\n","        char = np.random.choice(top_ch, p=p/p.sum())\n","        \n","        # return the encoded value of the predicted char and the hidden state\n","        return int2char[char], h"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bkde2wgM6GOa","colab_type":"text"},"source":["### Generating text from an initial string\n","\n","Typically you'll want to prime the network so you can build up a hidden state. Otherwise the network will start out generating characters at random. In general the first bunch of characters will be a little rough since it hasn't built up a long history of characters to predict from.\n","\n","> **Exercise:** Understand the following code! Nothing to do"]},{"cell_type":"code","metadata":{"id":"4Z3Z-Qqz6GOa","colab_type":"code","colab":{}},"source":["def sample(charLSTM_class, size, dict_c2int, prime='The', top_k=5):\n","    \n","    charLSTM_class.eval() # eval mode\n","    \n","    # First off, run through the prime characters\n","    chars = [ch for ch in prime]\n","    \n","    h = None\n","    for ch in prime:\n","        char, h = predict(charLSTM_class, ch, char2int, h, top_k=top_k)\n","\n","    chars.append(char)\n","    \n","    # Now pass in the previous character and get a new one\n","    for ii in range(size):\n","        char, h = predict(charLSTM_class, chars[-1], char2int, h, top_k=top_k)\n","        chars.append(char)\n","\n","    return ''.join(chars)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Np5xDgk86GOc","colab_type":"text"},"source":["Finally, lets take a look to the text that our model generates"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"Dqdh0zhd6GOd","colab_type":"code","outputId":"638406f9-0311-40e7-af20-4b13533b69d1","colab":{}},"source":["print(sample(my_charLSTM, 1000, char2int, prime='Anna', top_k=10))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Anna's alight, the consparated face the country's\n","ceasant, himself to be the last so lip would be serong the first was about the\n","point of the danger she sutponted to the partoratich,\" he panired, thought and straights,\n","athing, in his clarging contriticular to to him extlions with intervoute beed with his home,\n","and the pran firms and to go to him, he did not bligh but a girl\n","his\n","firse for a broor. The felence of the somethin had a feel of the went mettly. He heard that this they some speak, and was him, but their trianting old mounce screem, and the positing-seight in\n","the perslioss in how becoult not be done.\"\n","\n","\"Why, sisfical in the\n","dable with me countes, I know hims two whone? What my fealt feeling. You know as the ceasated, but's a\n","gastion he take\n","where the pate,\" he said,\n","lifting up.\n","\n","\"No, I know a pama to shait.\"\n","\n","Verencly dound it.\n","He would back\n","him he bad again of\n","her sin and chied,\n","thinking on the contritutural for thone bittle.\n","\n","\"It's not it?\"\n","\n","She was she down the counts who had not\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LXkL6vvn6GOe","colab_type":"text"},"source":["## Part V. Getting the best model (Optional)\n","\n","To set your hyperparameters to get the best performance, you'll want to watch the training and validation losses. If your training loss is much lower than the validation loss, you're overfitting. Increase regularization (more dropout) or use a smaller network. If the training and validation losses are close, you're underfitting so you can increase the size of the network.\n","\n","> **Exercise:** \n","> - Separate the text intro training text and validation text.\n","> - Modify the class CharLSTM_extended that monitors the validation loss during training\n","> - Save your model after every epoch (save the parameters in a different file for every epoch). To do so, include a method in the class to save the model parameters. Then, you call it after the end of every epoch.\n","> - Perform early stopping. I.e., load the parameters of the last epoch in which validation loss was decreasing.\n","\n","Here's some good advice from [Andrej Karpathy](https://github.com/karpathy/char-rnn#tips-and-tricks) on training the network.\n","\n","In order to save at every epoch, the following code can help. Here we are saving the parameters needed to create the same architecture, the hidden layer hyperparameters and the text characters.\n","\n","```python\n","\n","# change the name, for saving multiple files\n","model_name = 'rnn_' + str(e) + '_epoch.net'\n","\n","checkpoint = {'hidden_dim': my_charLSTM.hidden_dim,\n","              'n_layers': my_charLSTM.n_layers,\n","              'batch_size': my_charLSTM.batch_size,\n","              'sequence_length': my_charLSTM.seq_length,\n","              'state_dict': my_charLSTM.state_dict(),\n","              'chars': my_charLSTM.chars}\n","\n","with open(model_name, 'wb') as f:\n","    torch.save(checkpoint, f)\n","```\n","\n","Then, if you want to restore your model, you can use the following code\n","\n","```python\n","\n","# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n","with open('rnn_20_epoch.net', 'rb') as f:\n","    checkpoint = torch.load(f)\n","    \n","loaded = CharLSTM_extended(batch_size=checkpoint['batch_size'],seq_length=checkpoint['seq_length'], \n","                           chars=checkpoint['chars'],\n","                           hidden_dim=checkpoint['hidden_dim'], \n","                           n_layers=checkpoint['n_layers'])\n","loaded.load_state_dict(checkpoint['state_dict'])\n","```\n"]},{"cell_type":"code","metadata":{"id":"LRSKcPAI6GOf","colab_type":"code","colab":{}},"source":["#YOUR CODE HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HzB9WERX6GOi","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}